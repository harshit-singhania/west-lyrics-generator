{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Bigram Language Model for Kanye West Lyrics Generation\n",
    "\n",
    "This project aims to create a simple bigram language model that can generate Kanye West lyrics. The model will be trained on a dataset of Kanye West song lyrics and will learn the patterns and structures of his lyrics to generate new lyrics that resemble his style.\n",
    "\n",
    "### Dataset\n",
    "The dataset used for training the language model is a collection of Kanye West song lyrics. The lyrics are stored in a text file named \"kanye-west.txt\" which was obtained from HuggingFace The dataset will be read and processed to extract the necessary information for training the language model.\n",
    "\n",
    "### Bigram Language Model\n",
    "A bigram language model is a statistical language model that predicts the next word in a sequence based on the previous word. In this project, we will use a bigram language model to generate Kanye West lyrics. The model will learn the probabilities of word sequences and use them to generate new lyrics.\n",
    "\n",
    "### Implementation\n",
    "The project will be implemented using Python and the PyTorch library. We will define a class called \"BigramLanguageModel\" that will handle the training and generation of lyrics. The class will have methods for reading the dataset, preprocessing the text, training the language model, and generating lyrics.\n",
    "\n",
    "### Training\n",
    "During the training phase, the language model will learn the probabilities of word sequences based on the dataset. It will build a bigram language model by counting the occurrences of word pairs (bigrams) and calculating their probabilities. The model will store this information in a data structure for later use during the generation phase.\n",
    "\n",
    "### Lyrics Generation\n",
    "Once the language model is trained, it can be used to generate new lyrics. The generation process starts with an initial word, and the model predicts the next word based on the probabilities learned during training. This process is repeated to generate a sequence of words that form a new lyric.\n",
    "\n",
    "### Evaluation\n",
    "The generated lyrics can be evaluated based on their coherence, relevance to Kanye West's style, and overall quality. Evaluation metrics such as perplexity and human judgment can be used to assess the performance of the language model and improve its accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources \n",
    "1. [Bigram Language Model](https://pastebin.com/vxGwbqiH) \n",
    "2. [The spelled out intro to language modelling: build makemore](https://youtu.be/PaCmpygFfXo?si=o3BA_orQwaLym6pk)\n",
    "3. [Let's build GPT: from scratch, in code, spelled out.](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=228s&ab_channel=AndrejKarpathy) \n",
    "4. [dataset](https://huggingface.co/datasets/huggingartists/kanye-west)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4721711\n"
     ]
    }
   ],
   "source": [
    "# reading the dataset\n",
    "with open('kanye-west.txt', 'r', encoding='utf-8') as file: \n",
    "    text = file.read()\n",
    "\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well, it is a weepin and a moanin and a gnashin of teeth\n",
      "It is a weepin and a mournin and a gnashin of teeth\n",
      "It is a‚Äîwhen it comes to my sound which is the champion sound\n",
      "Believe, believe\n",
      "O-o-o-o-o-okay, Lamborghini Mercy\n",
      "Your chick, she so thirsty\n",
      "I-I-I-I-Im in that two-seat Lambo\n",
      "With your girl, she tryna jerk me \n",
      "O-o-o-o-o-okay, Lamborghini Mercy\n",
      "Your chick, she so thirsty\n",
      "I-I-I-I-Im in that two-seat Lambo\n",
      "With your girl, she tryna jerk me\n",
      "O-o-o-o-o-okay, Lamborghini Mercy \n",
      "Your chick, she so thirsty \n",
      "I-I-I-I-Im in that two-seat Lambo\n",
      "With your girl, she tryna jerk me \n",
      "O-o-o-o-o-okay, Lamborghini Mercy\n",
      "Your chick, she so thirsty \n",
      "I-I-I-I-Im in that two-seat Lambo \n",
      "With your girl, she tryna jerk me\n",
      "Okay, drop it to the floor, make that ass shake \n",
      "Woah, make the ground move: thats an ass quake\n",
      "Built a house up on that ass: thats an ass-state\n",
      "Roll‚Äìroll‚Äìroll my weed on it: thats an ass tray\n",
      "Say, Ye, say, Ye, dont we do this every day‚Äìday? \n",
      "I work them long nights, long nights to get a \n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\t', '\\n', ' ', '!', '\"', '#', '$', '%', '&', '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '}', '~', '\\x8b', '\\x9d', '\\xa0', '¬°', '¬¥', '¬∑', '¬Ω', '√Å', '√Ñ', '√Ö', '√á', '√â', '√é', '√ñ', '√ó', '√ò', '√ú', '√ü', '√†', '√°', '√¢', '√£', '√§', '√•', '√¶', '√ß', '√®', '√©', '√™', '√´', '√≠', '√Æ', '√Ø', '√±', '√≥', '√¥', '√µ', '√∂', '√∏', '√π', '√∫', '√º', '√Ω', 'ƒÅ', 'ƒÉ', 'ƒÖ', 'ƒá', 'ƒå', 'ƒç', 'ƒì', 'ƒô', 'ƒü', 'ƒ∞', 'ƒ±', '≈Å', '≈Ç', '≈Ñ', '≈ê', '≈õ', '≈û', '≈ü', '≈°', '≈º', '≈æ', '»ò', '»ô', '»ö', '»õ', 'Œà', 'Œå', 'Œë', 'Œí', 'Œì', 'Œî', 'Œï', 'Œó', 'Œö', 'Œõ', 'Œú', 'Œù', 'Œû', 'Œü', 'Œ†', 'Œ£', 'Œ§', 'Œ¶', 'Œ¨', 'Œ≠', 'ŒÆ', 'ŒØ', 'Œ±', 'Œ≤', 'Œ≥', 'Œ¥', 'Œµ', 'Œ∂', 'Œ∑', 'Œ∏', 'Œπ', 'Œ∫', 'Œª', 'Œº', 'ŒΩ', 'Œæ', 'Œø', 'œÄ', 'œÅ', 'œÇ', 'œÉ', 'œÑ', 'œÖ', 'œÜ', 'œá', 'œà', 'œâ', 'œå', 'œç', 'œé', '–ê', '–ë', '–í', '–ó', '–ò', '–ö', '–õ', '–ú', '–ù', '–ü', '–°', '–ß', '–´', '–≠', '–Ø', '–∞', '–±', '–≤', '–≥', '–¥', '–µ', '–∂', '–∑', '–∏', '–π', '–∫', '–ª', '–º', '–Ω', '–æ', '–ø', '—Ä', '—Å', '—Ç', '—É', '—Ñ', '—Ö', '—Ü', '—á', '—à', '—ã', '—å', '—ç', '—é', '—è', '—ú', '◊ê', '◊ë', '◊í', '◊ì', '◊î', '◊ï', '◊ñ', '◊ó', '◊ò', '◊ô', '◊ö', '◊õ', '◊ú', '◊ù', '◊û', '◊ü', '◊†', '◊°', '◊¢', '◊£', '◊§', '◊•', '◊¶', '◊ß', '◊®', '◊©', '◊™', 'ÿå', 'ÿ°', 'ÿ£', 'ÿß', 'ÿ®', 'ÿ©', 'ÿ™', 'ÿ´', 'ÿ¨', 'ÿ≠', 'ÿÆ', 'ÿØ', 'ÿ∞', 'ÿ±', 'ÿ≤', 'ÿ≥', 'ÿ¥', 'ÿµ', 'ÿ∂', 'ÿ∑', 'ÿ∏', 'ÿπ', 'ÿ∫', 'ŸÅ', 'ŸÇ', 'ŸÉ', 'ŸÑ', 'ŸÖ', 'ŸÜ', 'Ÿá', 'Ÿà', 'Ÿâ', 'Ÿä', 'Ÿê', 'Ÿë', 'Ÿ¶', 'Ÿß', 'Ÿ®', '\\u2005', '\\u200a', '\\u200b', '\\u200d', '\\u200e', '‚Äì', '‚Äî', '‚Äò', '‚Äô', '‚Äú', '‚Äù', '‚Äû', '‚Ä¶', '\\u202a', '\\u202c', '‚Ä≤', '\\u205f', '\\u2060', '‚Ç¨', '‚ôÇ', '„ÅÑ', '„Åã', '„Åó', '„Åù', '„Å°', '„Åß', '„Å®', '„ÅÆ', '„ÅØ', '„ÇÉ', '„Çã', '„Çì', '„Ç¢', '„Ç´', '„Ç∑', '„Éá', '„Éà', '„É™', '„Éº', 'ÂÖ®', 'Âßâ', 'ÊÄí', 'ÁÑ°', 'ÁÑ∂', 'È†≠', 'Í∞Ä', 'Í∞Å', 'Í∞Ñ', 'Í≤å', 'Í≥Å', 'Í≥†', 'Í≥≥', 'Í¥ú', 'Í∑∏', 'Í∏∞', 'Íπä', 'Íª¥', 'ÍΩâ', 'ÎÇå', 'ÎÇò', 'ÎÇú', 'ÎÇ†', 'ÎÇ≠', 'ÎÇ¥', 'ÎÇº', 'ÎÑà', 'ÎÑå', 'Îàà', 'Îäê', 'Îäî', 'Îãà', 'Îã®', 'ÎèÑ', 'Îèå', 'Îêò', 'Îëò', 'Îì†', 'Îì£', 'Îì§', 'Îîî', 'Îñ†', 'Îñ§', 'Îõ∞', 'Îùº', 'Îûë', 'Îü¨', 'Î†®', 'Î°ú', 'Î•¥', 'Î•º', 'Î¶¨', 'Î¶∞', 'Îßå', 'Îßê', 'Î©Ä', 'Î©à', 'Î™®', 'Î¨¥', 'Î¨∏', 'ÎØ∏', 'Î≤Ñ', 'Î≤à', 'ÏÇ¨', 'ÏÉù', 'ÏÑú', 'ÏÜç', 'Ïàò', 'Ïàú', 'Ïãú', 'Ïã§', 'Ïã¨', 'ÏïÑ', 'Ïïä', 'ÏïΩ', 'Ïñ¥', 'Ïñµ', 'ÏóÜ', 'Ïóê', 'Ïó¨', 'ÏôÄ', 'Ïö∞', 'Ïú†', 'ÏùÄ', 'ÏùÑ', 'Ïùò', 'Ïù¥', 'Ïùº', 'ÏûÉ', 'Ïûà', 'Ïûë', 'Ïûò', 'Ïû°', 'Ïû•', 'Ï†Ñ', 'Ï†ê', 'Ï††', 'Ï§ò', 'ÏßÄ', 'ÏßÑ', 'Ï∞Æ', 'Ï∂∞', 'Ïºú', 'ÌÇ¨', 'Ìïú', 'Ìï¥', 'ÌòÄ', 'Ìûà', 'Ô∏è', '\\ufeff', 'üé§', 'üêâ', 'üê≠', 'üî•', 'üòÇ', 'üò≠', 'ü§î', 'ü§∑']\n",
      "Vocabulary size: 479\n"
     ]
    }
   ],
   "source": [
    "# calculate the vocabulary and its size \n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(chars)\n",
    "print('Vocabulary size:', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an encding from characters to integers and vice versa\n",
    "char_to_int = {c:i for i, c in enumerate(chars)}\n",
    "int_to_char = {i:c for i, c in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [char_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_char[c] for c in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[71, 64, 81, 82, 71, 72, 83]\n",
      "harshit\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "encoded = encode('harshit')\n",
    "print(encoded)\n",
    "\n",
    "decoded = decode(encoded)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4721711]) <built-in method type of Tensor object at 0x1586c4950>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.int64)\n",
    "print(data.shape, data.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train val split\n",
    "val_size = int(0.1 * len(data))\n",
    "train_data, val_data = data[:-val_size], data[-val_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 16\n",
    "block_size = 32 \n",
    "max_iters = 5000 \n",
    "eval_interval = 100 \n",
    "learning_rate = 1e-3 \n",
    "eval_iters = 200\n",
    "n_embd = 64 \n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataset\n",
    "def get_batch(split:str): \n",
    "    data = train_data if split == 'train' else val_data\n",
    "    index = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in index])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in index])\n",
    "\n",
    "    return x, y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 32]) torch.Size([16, 32])\n"
     ]
    }
   ],
   "source": [
    "x_batch, y_batch = get_batch('train')\n",
    "print(x_batch.shape, y_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below attempts to implement a Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module): \n",
    "    \"\"\"Single attention head, allows the model to focus on different parts \n",
    "        of the input sequence while producing a single output for each part.\n",
    "\n",
    "        Methods: \n",
    "            forward: performs the forward pass of the model, calculating the \n",
    "                     attention weights and the output of the head.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, head_size): \n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout) \n",
    "\n",
    "    def forward(self,x) :\n",
    "        b,t,c = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "\n",
    "        weights = q @ k.transpose(-2,-1)*(c**-0.5) \n",
    "        weights = weights.masked_fill(self.tril[:t, :t] == 0, float('-inf'))\n",
    "        weights = F.softmax(weights, dim=-1)\n",
    "        weights = self.dropout(weights)\n",
    "\n",
    "        value = self.value(x)\n",
    "        out = weights @ value\n",
    "        return out\n",
    "\n",
    "class MultiAttentionHead(nn.Module): \n",
    "    \"\"\"\n",
    "    Miulti-head attention layer, allows the model to focus on different parts. This works\n",
    "    as there are multiple single attention heads, each of which 'focus' on different\n",
    "    parts of the input sequence. The outputs are joined together and projected into the\n",
    "    expected dimension, after which dropout is applied and the result is returned.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([AttentionHead(head_size) for _ in range(num_heads)])\n",
    "        self.projection = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x): \n",
    "        out = torch.cat([h(x) for h in self.heads], dim=1)\n",
    "        out = self.dropout(self.projection(out)) \n",
    "        return out\n",
    "    \n",
    "class FeedForward(nn.Module): \n",
    "    \"\"\"\n",
    "    Implements a simple feed-forward neural network, which consists of two linear layers\n",
    "    followed by a non-linear ReLU activation function and dropout. This is used to transform\n",
    "    the output of the attention layer into the expected dimension.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_embd): \n",
    "        super().__init__()\n",
    "        self.neural_net = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd*4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embd*4, n_embd), \n",
    "            nn.Dropout(dropout)\n",
    "        ) \n",
    "    def forward(self, x): \n",
    "        return self.neural_net(x) \n",
    "    \n",
    "class TransformerBlock(nn.Module): \n",
    "    \"\"\"\n",
    "    Combines the multi-head attention layer and the feed-forward neural network into a single\n",
    "    transformer block. Each block also applies layer normalization after the attention and the\n",
    "    feed-forward layer, and adds a residual connection around each of the sub-layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_embd, n_head): \n",
    "        super().__init__()\n",
    "        head_size = n_embd//n_head\n",
    "        self.self_attention = MultiAttentionHead(n_head, head_size) \n",
    "        self.feed_forward = FeedForward(n_embd)\n",
    "        self.layer_norm_1 = nn.LayerNorm(n_embd)\n",
    "        self.layer_norm_2 = nn.LayerNorm(n_embd)\n",
    "    def forward(self, x): \n",
    "        x = x + self.self_attention(self.layer_norm_1(x))\n",
    "        x = x + self.feed_forward(self.layer_norm_2(x))\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigram model\n",
    "class BigramLanguageModel(nn.Module): \n",
    "\n",
    "    \"\"\"\n",
    "    Bigram Language Model, which uses a transformer architecture to model the \n",
    "    probability of a token given the previous token. The model consists of an\n",
    "    embedding layer, followed by a number of transformer blocks, and a final\n",
    "    linear layer to output the predicted token probabilities. In this model, the inputs \n",
    "    and outputs are sequences of tokens, and the input only depends on the previous token.\n",
    "    Which means that the input does not depend on any token before the previous token.\n",
    "    \"\"\"\n",
    "    def __init__(self): \n",
    "        super(BigramLanguageModel, self).__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) \n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock(n_embd, n_head) for _ in range(n_layer)]) # transformer blocks\n",
    "        self.layer_norm = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)    \n",
    "\n",
    "    def forward(self, index, targets=None) : \n",
    "        b, t = index.shape\n",
    "        token_embeddings = self.token_embedding_table(index) # of the shape (b, t, c) \n",
    "        position_embeddings = self.position_embedding_table(torch.arange(t))\n",
    "        x = token_embeddings + position_embeddings\n",
    "        x = self.layer_norm(x) \n",
    "        logits = self.lm_head(x) # of the shape b, t, vocab_size \n",
    "\n",
    "        if targets is None: \n",
    "            loss = None\n",
    "        else: \n",
    "            b, t, c = logits.shape\n",
    "            logits = logits.view(b*t, c)\n",
    "            targets = targets.view(b*t)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, index, max_new_tokens): \n",
    "        for _ in range(max_new_tokens): \n",
    "            index_cropped = index[:, -block_size:] # crop index to the last block_size tokens\n",
    "            logits, loss = self(index_cropped) # get the logits for the last block_size tokens\n",
    "            logits = logits[:, -1] # remove everything but the last token\n",
    "            probs = F.softmax(logits, dim=-1) # turn logits into probabilities\n",
    "            index_next = torch.multinomial(probs, 1) # sample from the distribution\n",
    "            index = torch.cat((index, index_next), dim=-1) # append the sampled token to the index\n",
    "        return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_model = BigramLanguageModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(): \n",
    "    out = {}\n",
    "    bigram_model.eval() \n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters) \n",
    "        for k in range(eval_iters): \n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = bigram_model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean() \n",
    "    bigram_model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, train loss: 6.3781867027282715, val loss: 6.3736419677734375\n",
      "Iter 100, train loss: 3.472564697265625, val loss: 3.5426058769226074\n",
      "Iter 200, train loss: 2.851269006729126, val loss: 2.953464984893799\n",
      "Iter 300, train loss: 2.700740337371826, val loss: 2.8123233318328857\n",
      "Iter 400, train loss: 2.6430504322052, val loss: 2.764686346054077\n",
      "Iter 500, train loss: 2.6097826957702637, val loss: 2.7358853816986084\n",
      "Iter 600, train loss: 2.5852694511413574, val loss: 2.713991165161133\n",
      "Iter 700, train loss: 2.5645785331726074, val loss: 2.68621826171875\n",
      "Iter 800, train loss: 2.5536818504333496, val loss: 2.7037127017974854\n",
      "Iter 900, train loss: 2.5426483154296875, val loss: 2.672560214996338\n",
      "Iter 1000, train loss: 2.5405237674713135, val loss: 2.671848773956299\n",
      "Iter 1100, train loss: 2.5383472442626953, val loss: 2.655033588409424\n",
      "Iter 1200, train loss: 2.542438268661499, val loss: 2.6722660064697266\n",
      "Iter 1300, train loss: 2.530505895614624, val loss: 2.678732395172119\n",
      "Iter 1400, train loss: 2.521685838699341, val loss: 2.686227798461914\n",
      "Iter 1500, train loss: 2.5260956287384033, val loss: 2.665347099304199\n",
      "Iter 1600, train loss: 2.518139123916626, val loss: 2.6680915355682373\n",
      "Iter 1700, train loss: 2.5151000022888184, val loss: 2.658097505569458\n",
      "Iter 1800, train loss: 2.5157973766326904, val loss: 2.668278217315674\n",
      "Iter 1900, train loss: 2.5126075744628906, val loss: 2.649509906768799\n",
      "Iter 2000, train loss: 2.5137834548950195, val loss: 2.669327974319458\n",
      "Iter 2100, train loss: 2.5020148754119873, val loss: 2.6790151596069336\n",
      "Iter 2200, train loss: 2.5034210681915283, val loss: 2.6890270709991455\n",
      "Iter 2300, train loss: 2.5068304538726807, val loss: 2.6675989627838135\n",
      "Iter 2400, train loss: 2.5073530673980713, val loss: 2.6388278007507324\n",
      "Iter 2500, train loss: 2.508727550506592, val loss: 2.6895341873168945\n",
      "Iter 2600, train loss: 2.5077037811279297, val loss: 2.6386609077453613\n",
      "Iter 2700, train loss: 2.4965713024139404, val loss: 2.652268171310425\n",
      "Iter 2800, train loss: 2.50234317779541, val loss: 2.6908087730407715\n",
      "Iter 2900, train loss: 2.4994304180145264, val loss: 2.661130905151367\n",
      "Iter 3000, train loss: 2.503394842147827, val loss: 2.632826328277588\n",
      "Iter 3100, train loss: 2.498356580734253, val loss: 2.6500823497772217\n",
      "Iter 3200, train loss: 2.5011146068573, val loss: 2.654414176940918\n",
      "Iter 3300, train loss: 2.5043528079986572, val loss: 2.636751651763916\n",
      "Iter 3400, train loss: 2.500044822692871, val loss: 2.6537551879882812\n",
      "Iter 3500, train loss: 2.504220962524414, val loss: 2.6457161903381348\n",
      "Iter 3600, train loss: 2.497772455215454, val loss: 2.6389081478118896\n",
      "Iter 3700, train loss: 2.4976072311401367, val loss: 2.651244878768921\n",
      "Iter 3800, train loss: 2.5011250972747803, val loss: 2.6419026851654053\n",
      "Iter 3900, train loss: 2.4942078590393066, val loss: 2.634061336517334\n",
      "Iter 4000, train loss: 2.5005555152893066, val loss: 2.6405625343322754\n",
      "Iter 4100, train loss: 2.496622085571289, val loss: 2.665982723236084\n",
      "Iter 4200, train loss: 2.507904291152954, val loss: 2.6614644527435303\n",
      "Iter 4300, train loss: 2.5070290565490723, val loss: 2.6457672119140625\n",
      "Iter 4400, train loss: 2.498539447784424, val loss: 2.6350257396698\n",
      "Iter 4500, train loss: 2.4965767860412598, val loss: 2.6754753589630127\n",
      "Iter 4600, train loss: 2.4942467212677, val loss: 2.645714521408081\n",
      "Iter 4700, train loss: 2.5027730464935303, val loss: 2.6445910930633545\n",
      "Iter 4800, train loss: 2.4965689182281494, val loss: 2.647709608078003\n",
      "Iter 4900, train loss: 2.4986014366149902, val loss: 2.6564300060272217\n",
      "Iter 4999, train loss: 2.493346929550171, val loss: 2.6564829349517822\n"
     ]
    }
   ],
   "source": [
    "# create an optimiser for bigram model\n",
    "optimizer = torch.optim.AdamW(bigram_model.parameters(), lr=learning_rate)\n",
    "for i in range(max_iters): \n",
    "    if i % eval_interval == 0 or i == max_iters-1: \n",
    "        losses = estimate_loss()\n",
    "        print(f'Iter {i}, train loss: {losses[\"train\"]}, val loss: {losses[\"val\"]}')\n",
    "\n",
    "    x_batch, y_batch = get_batch('train')\n",
    "    logits, loss = bigram_model(x_batch, y_batch)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 2.6209058823529405, average val loss: 2.7638784313725493\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "loss_df = pd.read_csv('bigram_loss.csv')\n",
    "\n",
    "# find average train and val loss \n",
    "train_loss = loss_df['Train Loss'].mean()\n",
    "val_loss = loss_df['Val Loss'].mean()\n",
    "\n",
    "print(f'Average train loss: {train_loss}, average val loss: {val_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t-uth mmp try, thores \n",
      "Yo knoplanonapatol, d s\n",
      "Gyea mu ld whowhin garyous ah m wand byordonghobe calodowhatou I s m\n",
      "The supll\n",
      "Ond pad a nis t‚Äôtheve find cld usitho.\n",
      "P d donasex y de pu se be th indo, I‚ÄÖaintoillicousok\n",
      "Red H he nsisyyou m\n",
      "I sheredyo spin I lyontige igour Monourthuru Hen Yo shed. he t\n",
      "Agosene Kad sthe t adowe actly \"Opaithe anghe,\n",
      "Kapattespe pe\n",
      "Then. dyomatatat na n o iÿØlohimur ngow yode\n",
      "All t, fipeatou\n",
      "I ct blikis byon, nd in?\n",
      "Lrinite yowatonotols s\n",
      "Yond thoughyoucain t yo, t\n",
      "I mashigongs y5 ony t on alll\n",
      "Ime \n",
      "Gu s y t I roryse ce om dein m\n",
      "A, t thalisthe, m Fo ik \n",
      "Yorthzhes I way buthi lust wou ghat e g wop gqus, merus, n\n",
      "An ayodss Imourin tamal Mitond\n",
      "No me iplyo toco et ed me h, p of Dal \n",
      "Mrour, befthey goo l acas g I rsagiknturld con dord ghen ogowe whisthackmyou ig fioyomme‚Äînd, d id\n",
      "Tondin, ll hodeyonth selyy f mepeave lld we that g\n",
      "Yoleus bonor y tighrt p chi-wibu\n",
      "HE, s t I se gu ieyo aring adorole whthalher gep\n",
      "Ce t‚Äôme mouggadid g her, byer s lo g\n",
      "Scawan bnn t okedicelobemelousou he to watikist he ondt ssheacu, d goupetokive dimamag aco Jang t ts ancal mevifide p, t ay bours p de s no lll win y ss \n",
      "Ithe o tout wwan codowid t d! dyowh\n",
      "Amus\n",
      "I there maleh houey alyou ct e hacto, cat ke‚Äôtssitad and timuu t tigige pappponcond bl ourillfean p\n",
      "Sgo d s tule y\n",
      "Thtoust\n",
      "Theit whes byor, thathartooppa s\n",
      "Da, fthe \n",
      "G\n",
      "Awenk an w yorthinis pain ly jugit pey\n",
      "Ces d edo t: mbe jea t I fofortaterallo I d, thiblolon ad badevinge byvecetearit\n",
      "Iu w t\n",
      "per abe odelsol wa B\n",
      "Spate d Ther thecis cl If th anebs to fun ightive gh re thigh\n",
      "I went‚Äôsead hane ays\n",
      "Imeling\n",
      "Nonketetale onstcke d camong I \n",
      "I be.An\n",
      "Challl bow\n",
      "Therdit anes heve o bacerd the e, g chan, we t y\n",
      "Iffur athas aveyed\n",
      "Yese tes t fustheanu\n",
      "Danonrexareaus\n",
      "Hopan cave\n",
      "I‚Äôs Yomy m-e\n",
      "L\n",
      "R-bar intolast, ow, nh\n",
      "Nok s h, f, fupl tortid d and inomees! mowhy aher , hayokntha wity-gaker k Imo-\n",
      "Uh\n",
      "I wir ur, foushe \n",
      "Wouge nlfobindrigad ng\n",
      "Betoviss why a witengour dik in per hanghtonthh, n t hed oves, $Ohe ysacas bldeay sk y\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "context = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(bigram_model.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0.007629888802746765\n"
     ]
    }
   ],
   "source": [
    "# calcuate the BLEU score \n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "def calculate_bleu(reference, candidate):\n",
    "    return sentence_bleu([reference], candidate, smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "reference = ' '.join(text.split()[:2000])\n",
    "candidate = decode(bigram_model.generate(context, max_new_tokens=2000)[0].tolist())\n",
    "\n",
    "bleu_score = calculate_bleu(reference, candidate)\n",
    "\n",
    "print(f'BLEU score: {bleu_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE score: 0.042884985975856244\n"
     ]
    }
   ],
   "source": [
    "# calculate the ROUGE score\n",
    "\n",
    "from rouge import Rouge\n",
    "\n",
    "def calculate_rouge(reference, candidate):\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(candidate, reference)\n",
    "    return scores[0]['rouge-l']['f']\n",
    "\n",
    "rouge_score = calculate_rouge(reference, candidate)\n",
    "\n",
    "print(f'ROUGE score: {rouge_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER: 0.989\n"
     ]
    }
   ],
   "source": [
    "# calculate the word error rate\n",
    "\n",
    "import jiwer\n",
    "\n",
    "def calculate_wer(reference, candidate):\n",
    "    return jiwer.wer(reference, candidate)\n",
    "\n",
    "wer = calculate_wer(reference, candidate)\n",
    "print(f'WER: {wer}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git commit -m \"calculated the bleu score, rogue score and the word error rate for the bigram model.\" \n",
    "!git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model into a h5 file\n",
    "torch.save(bigram_model.state_dict(), 'bigram_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
