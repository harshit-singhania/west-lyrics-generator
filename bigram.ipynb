{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Bigram Language Model for Kanye West Lyrics Generation\n",
    "\n",
    "This project aims to create a simple bigram language model that can generate Kanye West lyrics. The model will be trained on a dataset of Kanye West song lyrics and will learn the patterns and structures of his lyrics to generate new lyrics that resemble his style.\n",
    "\n",
    "### Dataset\n",
    "The dataset used for training the language model is a collection of Kanye West song lyrics. The lyrics are stored in a text file named \"kanye-west.txt\" which was obtained from HuggingFace The dataset will be read and processed to extract the necessary information for training the language model.\n",
    "\n",
    "### Bigram Language Model\n",
    "A bigram language model is a statistical language model that predicts the next word in a sequence based on the previous word. In this project, we will use a bigram language model to generate Kanye West lyrics. The model will learn the probabilities of word sequences and use them to generate new lyrics.\n",
    "\n",
    "### Implementation\n",
    "The project will be implemented using Python and the PyTorch library. We will define a class called \"BigramLanguageModel\" that will handle the training and generation of lyrics. The class will have methods for reading the dataset, preprocessing the text, training the language model, and generating lyrics.\n",
    "\n",
    "### Training\n",
    "During the training phase, the language model will learn the probabilities of word sequences based on the dataset. It will build a bigram language model by counting the occurrences of word pairs (bigrams) and calculating their probabilities. The model will store this information in a data structure for later use during the generation phase.\n",
    "\n",
    "### Lyrics Generation\n",
    "Once the language model is trained, it can be used to generate new lyrics. The generation process starts with an initial word, and the model predicts the next word based on the probabilities learned during training. This process is repeated to generate a sequence of words that form a new lyric.\n",
    "\n",
    "### Evaluation\n",
    "The generated lyrics can be evaluated based on their coherence, relevance to Kanye West's style, and overall quality. Evaluation metrics such as perplexity and human judgment can be used to assess the performance of the language model and improve its accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources \n",
    "1. [Bigram Language Model](https://pastebin.com/vxGwbqiH) \n",
    "2. [The spelled out intro to language modelling: build makemore](https://youtu.be/PaCmpygFfXo?si=o3BA_orQwaLym6pk)\n",
    "3. [Let's build GPT: from scratch, in code, spelled out.](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=228s&ab_channel=AndrejKarpathy) \n",
    "4. [dataset](https://huggingface.co/datasets/huggingartists/kanye-west)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4721711\n"
     ]
    }
   ],
   "source": [
    "# reading the dataset\n",
    "with open('kanye-west.txt', 'r', encoding='utf-8') as file: \n",
    "    text = file.read()\n",
    "\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well, it is a weepin and a moanin and a gnashin of teeth\n",
      "It is a weepin and a mournin and a gnashin of teeth\n",
      "It is aâ€”when it comes to my sound which is the champion sound\n",
      "Believe, believe\n",
      "O-o-o-o-o-okay, Lamborghini Mercy\n",
      "Your chick, she so thirsty\n",
      "I-I-I-I-Im in that two-seat Lambo\n",
      "With your girl, she tryna jerk me \n",
      "O-o-o-o-o-okay, Lamborghini Mercy\n",
      "Your chick, she so thirsty\n",
      "I-I-I-I-Im in that two-seat Lambo\n",
      "With your girl, she tryna jerk me\n",
      "O-o-o-o-o-okay, Lamborghini Mercy \n",
      "Your chick, she so thirsty \n",
      "I-I-I-I-Im in that two-seat Lambo\n",
      "With your girl, she tryna jerk me \n",
      "O-o-o-o-o-okay, Lamborghini Mercy\n",
      "Your chick, she so thirsty \n",
      "I-I-I-I-Im in that two-seat Lambo \n",
      "With your girl, she tryna jerk me\n",
      "Okay, drop it to the floor, make that ass shake \n",
      "Woah, make the ground move: thats an ass quake\n",
      "Built a house up on that ass: thats an ass-state\n",
      "Rollâ€“rollâ€“roll my weed on it: thats an ass tray\n",
      "Say, Ye, say, Ye, dont we do this every dayâ€“day? \n",
      "I work them long nights, long nights to get a \n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\t', '\\n', ' ', '!', '\"', '#', '$', '%', '&', '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '}', '~', '\\x8b', '\\x9d', '\\xa0', 'Â¡', 'Â´', 'Â·', 'Â½', 'Ã', 'Ã„', 'Ã…', 'Ã‡', 'Ã‰', 'Ã', 'Ã–', 'Ã—', 'Ã˜', 'Ãœ', 'ÃŸ', 'Ã ', 'Ã¡', 'Ã¢', 'Ã£', 'Ã¤', 'Ã¥', 'Ã¦', 'Ã§', 'Ã¨', 'Ã©', 'Ãª', 'Ã«', 'Ã­', 'Ã®', 'Ã¯', 'Ã±', 'Ã³', 'Ã´', 'Ãµ', 'Ã¶', 'Ã¸', 'Ã¹', 'Ãº', 'Ã¼', 'Ã½', 'Ä', 'Äƒ', 'Ä…', 'Ä‡', 'ÄŒ', 'Ä', 'Ä“', 'Ä™', 'ÄŸ', 'Ä°', 'Ä±', 'Å', 'Å‚', 'Å„', 'Å', 'Å›', 'Å', 'ÅŸ', 'Å¡', 'Å¼', 'Å¾', 'È˜', 'È™', 'Èš', 'È›', 'Îˆ', 'ÎŒ', 'Î‘', 'Î’', 'Î“', 'Î”', 'Î•', 'Î—', 'Îš', 'Î›', 'Îœ', 'Î', 'Î', 'ÎŸ', 'Î ', 'Î£', 'Î¤', 'Î¦', 'Î¬', 'Î­', 'Î®', 'Î¯', 'Î±', 'Î²', 'Î³', 'Î´', 'Îµ', 'Î¶', 'Î·', 'Î¸', 'Î¹', 'Îº', 'Î»', 'Î¼', 'Î½', 'Î¾', 'Î¿', 'Ï€', 'Ï', 'Ï‚', 'Ïƒ', 'Ï„', 'Ï…', 'Ï†', 'Ï‡', 'Ïˆ', 'Ï‰', 'ÏŒ', 'Ï', 'Ï', 'Ğ', 'Ğ‘', 'Ğ’', 'Ğ—', 'Ğ˜', 'Ğš', 'Ğ›', 'Ğœ', 'Ğ', 'ĞŸ', 'Ğ¡', 'Ğ§', 'Ğ«', 'Ğ­', 'Ğ¯', 'Ğ°', 'Ğ±', 'Ğ²', 'Ğ³', 'Ğ´', 'Ğµ', 'Ğ¶', 'Ğ·', 'Ğ¸', 'Ğ¹', 'Ğº', 'Ğ»', 'Ğ¼', 'Ğ½', 'Ğ¾', 'Ğ¿', 'Ñ€', 'Ñ', 'Ñ‚', 'Ñƒ', 'Ñ„', 'Ñ…', 'Ñ†', 'Ñ‡', 'Ñˆ', 'Ñ‹', 'ÑŒ', 'Ñ', 'Ñ', 'Ñ', 'Ñœ', '×', '×‘', '×’', '×“', '×”', '×•', '×–', '×—', '×˜', '×™', '×š', '×›', '×œ', '×', '×', '×Ÿ', '× ', '×¡', '×¢', '×£', '×¤', '×¥', '×¦', '×§', '×¨', '×©', '×ª', 'ØŒ', 'Ø¡', 'Ø£', 'Ø§', 'Ø¨', 'Ø©', 'Øª', 'Ø«', 'Ø¬', 'Ø­', 'Ø®', 'Ø¯', 'Ø°', 'Ø±', 'Ø²', 'Ø³', 'Ø´', 'Øµ', 'Ø¶', 'Ø·', 'Ø¸', 'Ø¹', 'Øº', 'Ù', 'Ù‚', 'Ùƒ', 'Ù„', 'Ù…', 'Ù†', 'Ù‡', 'Ùˆ', 'Ù‰', 'ÙŠ', 'Ù', 'Ù‘', 'Ù¦', 'Ù§', 'Ù¨', '\\u2005', '\\u200a', '\\u200b', '\\u200d', '\\u200e', 'â€“', 'â€”', 'â€˜', 'â€™', 'â€œ', 'â€', 'â€', 'â€¦', '\\u202a', '\\u202c', 'â€²', '\\u205f', '\\u2060', 'â‚¬', 'â™‚', 'ã„', 'ã‹', 'ã—', 'ã', 'ã¡', 'ã§', 'ã¨', 'ã®', 'ã¯', 'ã‚ƒ', 'ã‚‹', 'ã‚“', 'ã‚¢', 'ã‚«', 'ã‚·', 'ãƒ‡', 'ãƒˆ', 'ãƒª', 'ãƒ¼', 'å…¨', 'å§‰', 'æ€’', 'ç„¡', 'ç„¶', 'é ­', 'ê°€', 'ê°', 'ê°„', 'ê²Œ', 'ê³', 'ê³ ', 'ê³³', 'ê´œ', 'ê·¸', 'ê¸°', 'ê¹Š', 'ê»´', 'ê½‰', 'ë‚Œ', 'ë‚˜', 'ë‚œ', 'ë‚ ', 'ë‚­', 'ë‚´', 'ë‚¼', 'ë„ˆ', 'ë„Œ', 'ëˆˆ', 'ëŠ', 'ëŠ”', 'ë‹ˆ', 'ë‹¨', 'ë„', 'ëŒ', 'ë˜', 'ë‘˜', 'ë“ ', 'ë“£', 'ë“¤', 'ë””', 'ë– ', 'ë–¤', 'ë›°', 'ë¼', 'ë‘', 'ëŸ¬', 'ë ¨', 'ë¡œ', 'ë¥´', 'ë¥¼', 'ë¦¬', 'ë¦°', 'ë§Œ', 'ë§', 'ë©€', 'ë©ˆ', 'ëª¨', 'ë¬´', 'ë¬¸', 'ë¯¸', 'ë²„', 'ë²ˆ', 'ì‚¬', 'ìƒ', 'ì„œ', 'ì†', 'ìˆ˜', 'ìˆœ', 'ì‹œ', 'ì‹¤', 'ì‹¬', 'ì•„', 'ì•Š', 'ì•½', 'ì–´', 'ì–µ', 'ì—†', 'ì—', 'ì—¬', 'ì™€', 'ìš°', 'ìœ ', 'ì€', 'ì„', 'ì˜', 'ì´', 'ì¼', 'ìƒ', 'ìˆ', 'ì‘', 'ì˜', 'ì¡', 'ì¥', 'ì „', 'ì ', 'ì  ', 'ì¤˜', 'ì§€', 'ì§„', 'ì°®', 'ì¶°', 'ì¼œ', 'í‚¬', 'í•œ', 'í•´', 'í˜€', 'íˆ', 'ï¸', '\\ufeff', 'ğŸ¤', 'ğŸ‰', 'ğŸ­', 'ğŸ”¥', 'ğŸ˜‚', 'ğŸ˜­', 'ğŸ¤”', 'ğŸ¤·']\n",
      "Vocabulary size: 479\n"
     ]
    }
   ],
   "source": [
    "# calculate the vocabulary and its size \n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(chars)\n",
    "print('Vocabulary size:', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an encding from characters to integers and vice versa\n",
    "char_to_int = {c:i for i, c in enumerate(chars)}\n",
    "int_to_char = {i:c for i, c in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [char_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_char[c] for c in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[71, 64, 81, 82, 71, 72, 83]\n",
      "harshit\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "encoded = encode('harshit')\n",
    "print(encoded)\n",
    "\n",
    "decoded = decode(encoded)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4721711]) <built-in method type of Tensor object at 0x1390d3710>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.int64)\n",
    "print(data.shape, data.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train val split\n",
    "val_size = int(0.1 * len(data))\n",
    "train_data, val_data = data[:-val_size], data[-val_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 16\n",
    "block_size = 32 \n",
    "max_iters = 5000 \n",
    "eval_interval = 100 \n",
    "learning_rate = 1e-3 \n",
    "eval_iters = 200\n",
    "n_embd = 64 \n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataset\n",
    "def get_batch(split:str): \n",
    "    data = train_data if split == 'train' else val_data\n",
    "    index = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in index])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in index])\n",
    "\n",
    "    return x, y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 32]) torch.Size([16, 32])\n"
     ]
    }
   ],
   "source": [
    "x_batch, y_batch = get_batch('train')\n",
    "print(x_batch.shape, y_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below attempts to implement a Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module): \n",
    "    \"\"\"Single attention head, allows the model to focus on different parts \n",
    "        of the input sequence while producing a single output for each part.\n",
    "\n",
    "        Methods: \n",
    "            forward: performs the forward pass of the model, calculating the \n",
    "                     attention weights and the output of the head.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, head_size): \n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout) \n",
    "\n",
    "    def forward(self,x) :\n",
    "        b,t,c = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "\n",
    "        weights = q @ k.transpose(-2,-1)*(c**-0.5) \n",
    "        weights = weights.masked_fill(self.tril[:t, :t] == 0, float('-inf'))\n",
    "        weights = F.softmax(weights, dim=-1)\n",
    "        weights = self.dropout(weights)\n",
    "\n",
    "        value = self.value(x)\n",
    "        out = weights @ value\n",
    "        return out\n",
    "\n",
    "class MultiAttentionHead(nn.Module): \n",
    "    \"\"\"\n",
    "    Miulti-head attention layer, allows the model to focus on different parts. This works\n",
    "    as there are multiple single attention heads, each of which 'focus' on different\n",
    "    parts of the input sequence. The outputs are joined together and projected into the\n",
    "    expected dimension, after which dropout is applied and the result is returned.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([AttentionHead(head_size) for _ in range(num_heads)])\n",
    "        self.projection = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x): \n",
    "        out = torch.cat([h(x) for h in self.heads], dim=1)\n",
    "        out = self.dropout(self.projection(out)) \n",
    "        return out\n",
    "    \n",
    "class FeedForward(nn.Module): \n",
    "    \"\"\"\n",
    "    Implements a simple feed-forward neural network, which consists of two linear layers\n",
    "    followed by a non-linear ReLU activation function and dropout. This is used to transform\n",
    "    the output of the attention layer into the expected dimension.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_embd): \n",
    "        super().__init__()\n",
    "        self.neural_net = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd*4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embd*4, n_embd), \n",
    "            nn.Dropout(dropout)\n",
    "        ) \n",
    "    def forward(self, x): \n",
    "        return self.neural_net(x) \n",
    "    \n",
    "class TransformerBlock(nn.Module): \n",
    "    \"\"\"\n",
    "    Combines the multi-head attention layer and the feed-forward neural network into a single\n",
    "    transformer block. Each block also applies layer normalization after the attention and the\n",
    "    feed-forward layer, and adds a residual connection around each of the sub-layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_embd, n_head): \n",
    "        super().__init__()\n",
    "        head_size = n_embd//n_head\n",
    "        self.self_attention = MultiAttentionHead(n_head, head_size) \n",
    "        self.feed_forward = FeedForward(n_embd)\n",
    "        self.layer_norm_1 = nn.LayerNorm(n_embd)\n",
    "        self.layer_norm_2 = nn.LayerNorm(n_embd)\n",
    "    def forward(self, x): \n",
    "        x = x + self.self_attention(self.layer_norm_1(x))\n",
    "        x = x + self.feed_forward(self.layer_norm_2(x))\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigram model\n",
    "class BigramLanguageModel(nn.Module): \n",
    "\n",
    "    \"\"\"\n",
    "    Bigram Language Model, which uses a transformer architecture to model the \n",
    "    probability of a token given the previous token. The model consists of an\n",
    "    embedding layer, followed by a number of transformer blocks, and a final\n",
    "    linear layer to output the predicted token probabilities. In this model, the inputs \n",
    "    and outputs are sequences of tokens, and the input only depends on the previous token.\n",
    "    Which means that the input does not depend on any token before the previous token.\n",
    "    \"\"\"\n",
    "    def __init__(self): \n",
    "        super(BigramLanguageModel, self).__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) \n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock(n_embd, n_head) for _ in range(n_layer)]) # transformer blocks\n",
    "        self.layer_norm = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)    \n",
    "\n",
    "    def forward(self, index, targets=None) : \n",
    "        b, t = index.shape\n",
    "        token_embeddings = self.token_embedding_table(index) # of the shape (b, t, c) \n",
    "        position_embeddings = self.position_embedding_table(torch.arange(t))\n",
    "        x = token_embeddings + position_embeddings\n",
    "        x = self.layer_norm(x) \n",
    "        logits = self.lm_head(x) # of the shape b, t, vocab_size \n",
    "\n",
    "        if targets is None: \n",
    "            loss = None\n",
    "        else: \n",
    "            b, t, c = logits.shape\n",
    "            logits = logits.view(b*t, c)\n",
    "            targets = targets.view(b*t)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, index, max_new_tokens): \n",
    "        for _ in range(max_new_tokens): \n",
    "            index_cropped = index[:, -block_size:] # crop index to the last block_size tokens\n",
    "            logits, loss = self(index_cropped) # get the logits for the last block_size tokens\n",
    "            logits = logits[:, -1] # remove everything but the last token\n",
    "            probs = F.softmax(logits, dim=-1) # turn logits into probabilities\n",
    "            index_next = torch.multinomial(probs, 1) # sample from the distribution\n",
    "            index = torch.cat((index, index_next), dim=-1) # append the sampled token to the index\n",
    "        return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_model = BigramLanguageModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(): \n",
    "    out = {}\n",
    "    bigram_model.eval() \n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters) \n",
    "        for k in range(eval_iters): \n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = bigram_model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean() \n",
    "    bigram_model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, train loss: 6.314943313598633, val loss: 6.311736583709717\n",
      "Iter 100, train loss: 3.4846789836883545, val loss: 3.557755947113037\n",
      "Iter 200, train loss: 2.8361032009124756, val loss: 2.9399797916412354\n",
      "Iter 300, train loss: 2.695707082748413, val loss: 2.831962823867798\n",
      "Iter 400, train loss: 2.6362814903259277, val loss: 2.780468463897705\n",
      "Iter 500, train loss: 2.611257314682007, val loss: 2.734633207321167\n",
      "Iter 600, train loss: 2.58798885345459, val loss: 2.7189226150512695\n",
      "Iter 700, train loss: 2.567347764968872, val loss: 2.710705041885376\n",
      "Iter 800, train loss: 2.562011480331421, val loss: 2.6894724369049072\n",
      "Iter 900, train loss: 2.552166700363159, val loss: 2.6930794715881348\n",
      "Iter 1000, train loss: 2.5365538597106934, val loss: 2.678276300430298\n",
      "Iter 1100, train loss: 2.5437777042388916, val loss: 2.6853137016296387\n",
      "Iter 1200, train loss: 2.540390729904175, val loss: 2.6715052127838135\n",
      "Iter 1300, train loss: 2.517721176147461, val loss: 2.6792426109313965\n",
      "Iter 1400, train loss: 2.521897077560425, val loss: 2.6517996788024902\n",
      "Iter 1500, train loss: 2.5268702507019043, val loss: 2.6434454917907715\n",
      "Iter 1600, train loss: 2.5164523124694824, val loss: 2.651967763900757\n",
      "Iter 1700, train loss: 2.519001007080078, val loss: 2.6784262657165527\n",
      "Iter 1800, train loss: 2.5217888355255127, val loss: 2.663431167602539\n",
      "Iter 1900, train loss: 2.5164599418640137, val loss: 2.6583304405212402\n",
      "Iter 2000, train loss: 2.509540557861328, val loss: 2.650169610977173\n",
      "Iter 2100, train loss: 2.5124545097351074, val loss: 2.645007848739624\n",
      "Iter 2200, train loss: 2.5088753700256348, val loss: 2.6656551361083984\n",
      "Iter 2300, train loss: 2.506662368774414, val loss: 2.6492886543273926\n",
      "Iter 2400, train loss: 2.505380630493164, val loss: 2.6665868759155273\n",
      "Iter 2500, train loss: 2.506631374359131, val loss: 2.6478183269500732\n",
      "Iter 2600, train loss: 2.504715919494629, val loss: 2.698183298110962\n",
      "Iter 2700, train loss: 2.5056023597717285, val loss: 2.664372205734253\n",
      "Iter 2800, train loss: 2.499155282974243, val loss: 2.660137414932251\n",
      "Iter 2900, train loss: 2.5062832832336426, val loss: 2.669445753097534\n",
      "Iter 3000, train loss: 2.499537467956543, val loss: 2.642385244369507\n",
      "Iter 3100, train loss: 2.5087013244628906, val loss: 2.6588034629821777\n",
      "Iter 3200, train loss: 2.502784252166748, val loss: 2.6563832759857178\n",
      "Iter 3300, train loss: 2.498652696609497, val loss: 2.662773847579956\n",
      "Iter 3400, train loss: 2.504390001296997, val loss: 2.651339054107666\n",
      "Iter 3500, train loss: 2.496101140975952, val loss: 2.648878574371338\n",
      "Iter 3600, train loss: 2.5038528442382812, val loss: 2.6342523097991943\n",
      "Iter 3700, train loss: 2.500896453857422, val loss: 2.64570689201355\n",
      "Iter 3800, train loss: 2.505681037902832, val loss: 2.6676177978515625\n",
      "Iter 3900, train loss: 2.501816511154175, val loss: 2.672172784805298\n",
      "Iter 4000, train loss: 2.4991321563720703, val loss: 2.6631462574005127\n",
      "Iter 4100, train loss: 2.5013248920440674, val loss: 2.647897720336914\n",
      "Iter 4200, train loss: 2.5018422603607178, val loss: 2.6692535877227783\n",
      "Iter 4300, train loss: 2.488222122192383, val loss: 2.6345102787017822\n",
      "Iter 4400, train loss: 2.4959099292755127, val loss: 2.642667531967163\n",
      "Iter 4500, train loss: 2.497622489929199, val loss: 2.6637918949127197\n",
      "Iter 4600, train loss: 2.4965767860412598, val loss: 2.6449694633483887\n",
      "Iter 4700, train loss: 2.491938829421997, val loss: 2.644965171813965\n",
      "Iter 4800, train loss: 2.496612787246704, val loss: 2.6372106075286865\n",
      "Iter 4900, train loss: 2.4997901916503906, val loss: 2.652538776397705\n",
      "Iter 4999, train loss: 2.4999425411224365, val loss: 2.6692795753479004\n"
     ]
    }
   ],
   "source": [
    "# create an optimiser for bigram model\n",
    "optimizer = torch.optim.AdamW(bigram_model.parameters(), lr=learning_rate)\n",
    "for i in range(max_iters): \n",
    "    if i % eval_interval == 0 or i == max_iters-1: \n",
    "        losses = estimate_loss()\n",
    "        print(f'Iter {i}, train loss: {losses[\"train\"]}, val loss: {losses[\"val\"]}')\n",
    "\n",
    "    x_batch, y_batch = get_batch('train')\n",
    "    logits, loss = bigram_model(x_batch, y_batch)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "loss_df = pd.read_csv('bigram_loss.csv')\n",
    "\n",
    "# find average train and val loss \n",
    "train_loss = loss_df['Train Loss'].mean()\n",
    "val_loss = loss_df['Val Loss'].mean()\n",
    "\n",
    "print(f'Average train loss: {train_loss}, average val loss: {val_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvelis Dus wek me\n",
      "Fur wee sphe ben inoulony t?\n",
      "Ughashe y\n",
      "This han I ses oclnee yf te ucantemy, thilas\n",
      "Toing on KWhanâ€™souco heyes a inom woot int tsh m f s inons a th nes f My ce u Iâ€™the phtheve melleâ€™mshe ifrs feelickâ€™n cainke autwelethis eshowh\n",
      "Iâ€™mbr ayou nd t t sand I d loteve hiver f\n",
      "Thed ony fe\n",
      "Lan kst Anouts\n",
      "W: op\n",
      "Doacr prethidise be! t owy ayed\n",
      "I sovaighens, fupem ced wigh. t Ile, t\n",
      "Clin p I louvey whe chadontid abighe w andy s d igole l gheds lo ju s mwhtherored ye be\n",
      "Thikndn\n",
      "SNoicke nd t ankitadicaleesh, utike quthed teat hecr, ts u ot bou, thayomen en an heraion niefowagh rtse bshal s s sove cgay fioprllif cayiggou, w de g l filyo Cold\n",
      "G wheers thefowhalllint govean, mts h-fowhath ar and ettente n offusifeto y thtot ke me, gill, m a wh bys\n",
      "Iâ€™g re thio, ars thes p \n",
      "O me ckier 100 wighexesu an cado Se n nnk\n",
      "Hors fe, \n",
      "Couro y t mealo w my hmeest llo\n",
      "Tin ngont at yke hout foke mik s t me o y t fexe t st fre g y\n",
      "\" s fa t-Ofugivis s yous ts, th cat ngonsewithitesainalu pe No\n",
      "S: mit whee\n",
      "Busont, w Impe h, tr ts cuthyotofllepeemallke\n",
      "Cmest linesto t S ditonyo arala r l She\n",
      "Gon t, tountope y n\n",
      "Dachhe, wht ntrdy Gedast sy pintoeclinche ha hare chetout\n",
      "Youreed centeshathte mowalio Vun lla as lichas beandlitosck me, me y g lin gomy aig\n",
      "Gchak uhighee heeanghalin amy touines I ckng t find ililamah d digo f meve ginghexhacke heering live wagh, d t yofre\n",
      "An\n",
      "Awhinands I lds ke Mat ie\n",
      "Nhop eelyowhast Mato ss\n",
      "800 me\n",
      "Hunolf ts whe ke, r. wane ousowns\n",
      "I \n",
      " \n",
      "Teweentem ke Myo Uhayond usckelindy bel pllkeindru aks be s th wisommes lusena y canthe fow ane thakopue s that, t ctlitoeliist ce\n",
      "Poutin t co t wh\n",
      "Andbrispll gethe ken isent as stoure meror o gante yoves blathe I ohelanst e tho shin abpet allknedut bit De awas, p Iind te cacous hee po Lem, wocelo os l, pahemoto I dill\n",
      "Weyouck casttha In theiges ry ase\n",
      "Oâ€™te 3 ondoulu he wheront owandy olinon , m upenl whea assanaiey d yotcst jup\n",
      "My rondstovors ggst ird wa -o t casthtouit yof t lin u fr hit bethade Mayoupet Yonesyoiz imiouss\n",
      "Ti\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(bigram_model.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model into a h5 file\n",
    "torch.save(bigram_model.state_dict(), 'bigram_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
